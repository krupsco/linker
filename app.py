import os
import re
import io
import json
import zipfile
from typing import Dict, List, Tuple

import streamlit as st

# --- OpenAI SDK (>=1.0) ---
try:
    from openai import OpenAI
except ImportError:
    st.error("Brak pakietu 'openai'. Upewnij siƒô, ≈ºe jest w requirements.txt")
    st.stop()

# ========== Konfiguracja ==========
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")  # üöÄ szybszy model
MAX_CHARS_PER_CHUNK = 6500   # bezpieczny limit na chunk (przybli≈ºenie)
CHUNK_OVERLAP = 200          # miƒôkkie ‚Äûzak≈Çadki‚Äù miƒôdzy chunkami

# Pobranie klucza z secrets Streamlita
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))

if not OPENAI_API_KEY:
    st.warning("‚ö†Ô∏è Nie znaleziono OPENAI_API_KEY w secrets. "
               "Dodaj go do .streamlit/secrets.toml lub zmiennej ≈õrodowiskowej.")
client = OpenAI(api_key=OPENAI_API_KEY)


# ========== Prompty ==========
SYSTEM_PROMPT_BASE = """Jeste≈õ asystentem-edytorem tekst√≥w naukowych w jƒôzyku polskim.
Twoje zadanie: dodaƒá linki Obsidiana w nawiasach [[ ]] do PODANEGO TEKSTU, bez redakcji tre≈õci.
Stosuj ≈õcis≈Çe zasady:

1) Linkujemy WSZYSTKIE kluczowe nazwy w≈Çasne: osoby (historyczne, dynastie), budowle (zamki, pa≈Çace, ko≈õcio≈Çy, wie≈ºe), ulice/du≈ºe place/topografiƒô,
   instytucje (sejm, urzƒôdy, komisje, muzea), wydarzenia (wojny, unie, bitwy, konfederacje, elekcje).
2) Je≈õli forma w tek≈õcie jest odmieniona (nie w mianowniku), u≈ºyj aliasu: [[LEMMA | forma_z_tekstu]].
   Je≈õli forma jest r√≥wna mianownikowi, u≈ºyj [[LEMMA]].
3) LEMMA (mianownik) dobieraj nastƒôpujƒÖco:
   - osoby: pe≈Çne ‚ÄûImiƒô Nazwisko‚Äù + numeracja/dynastia (np. ‚ÄûZygmunt III Waza‚Äù, ‚ÄûJanusz I Starszy‚Äù).
   - rody/dynastie: liczba mnoga (‚ÄûWazowie‚Äù, ‚ÄûPiastowie mazowieccy‚Äù).
   - budowle: oficjalna polska nazwa (np. ‚ÄûZamek Kr√≥lewski w Warszawie‚Äù, ‚ÄûWie≈ºa Grodzka‚Äù, ‚ÄûDw√≥r Wielki‚Äù).
   - wydarzenia: forma s≈Çownikowa (np. ‚ÄûUnia lubelska‚Äù, ‚ÄûPotop szwedzki‚Äù, ‚ÄûBitwa pod K≈Çuszynem‚Äù).
   - instytucje i topografia: forma s≈Çownikowa z kwalifikatorami miejsca (np. ‚ÄûKolegiata ≈õw. Jana w Warszawie‚Äù, ‚ÄûPlac Zamkowy‚Äù).
4) NIE modyfikuj istniejƒÖcych [[wikilink√≥w]], link√≥w Markdown, obrazk√≥w `![]()`, tytu≈Ç√≥w obraz√≥w/cytat√≥w w nawiasach, ani tre≈õci blok√≥w kodu.
5) Zachowaj oryginalne odstƒôpy, interpunkcjƒô i diakrytykƒô. Nie skracaj, nie poprawiaj stylu.
6) Je≈ºeli dany byt by≈Ç ju≈º zmapowany w KNOWN_ENTITIES (lista poni≈ºej), u≈ºywaj dok≈Çadnie tej lemmy (sp√≥jno≈õƒá ca≈Çego dokumentu).
7) Nie linkuj zwyk≈Çych rzeczownik√≥w pospolitych ani zbyt og√≥lnych pojƒôƒá.

ZWROT:
- Najpierw zwr√≥ƒá WY≈ÅƒÑCZNIE przetworzony tekst.
- Nastƒôpnie w nowej linii daj separator:
-----ENTITY_MAP_JSON-----
i JSON z listƒÖ wykrytych/aktualizowanych byt√≥w (pole "entities": [{surface, lemma, type}...]).
- Zako≈Ñcz:
-----END-----

KNOWN_ENTITIES (priorytet sp√≥jno≈õci):
"""

# ‚ûï NOWE: Prompt do minimalnej korekty OCR/PDF (bez redakcji merytorycznej/stylowej)
SYSTEM_PROMPT_OCR = """Jeste≈õ asystentem do minimalnej korekty tekstu po OCR/PDF w jƒôzyku polskim.
Twoje zadanie: usunƒÖƒá wy≈ÇƒÖcznie ARTEFAKTY formatowania, zachowujƒÖc tre≈õƒá i styl bez redakcji merytorycznej.
≈öci≈õle stosuj poni≈ºsze zasady:

A) Zachowaj oryginalnƒÖ strukturƒô dokumentu: nag≈Ç√≥wki, akapity, listy, cytaty. Nie zmieniaj kolejno≈õci tre≈õci.
B) NIE modyfikuj istniejƒÖcych [[wikilink√≥w]], link√≥w Markdown, obrazk√≥w `![]()`, ani blok√≥w kodu (```...``` lub `...`).
C) Usu≈Ñ typowe artefakty OCR/PDF:
   - dzielenie wyraz√≥w twardymi ≈ÇƒÖcznikami na ko≈Ñcu linii (np. ‚Äûtechno-\nlogie‚Äù ‚Üí ‚Äûtechnologie‚Äù),
   - niezamierzone z≈Çamania w ≈õrodku zdania/wyrazu,
   - zduplikowane spacje, przypadkowe tabulatory,
   - przypadkowe ≈õmieciowe znaki (np. pojedyncze ‚Äû.‚Äù na osobnych liniach, losowe znaki kontrolne),
   - ‚Äûp≈ÇywajƒÖce‚Äù nag≈Ç√≥wki i podpisy, je≈õli ewidentnie nale≈ºƒÖ do sƒÖsiedniego akapitu.
D) Nie poprawiaj liter√≥wek ani interpunkcji, chyba ≈ºe sƒÖ skutkiem oczywistego artefaktu OCR (np. ‚ÄûwRzymie‚Äù ‚Üí ‚Äûw Rzymie‚Äù).
E) Nie zmieniaj sensu, nie skracaj, nie dopisuj.

ZWROT:
- Zwr√≥ƒá WY≈ÅƒÑCZNIE oczyszczony tekst (bez dodatkowych komentarzy).
"""

# typowa heurystyka wykrywania blok√≥w kodu/obrazk√≥w/link√≥w: nie zmieniamy ich
FENCE_RE = re.compile(r"(```.*?```|`.*?`)", re.DOTALL)
MARKDOWN_LINK_OR_IMAGE_RE = re.compile(r"(!?\[[^\]]*\]\([^)]+\))")


# ========== Pomocnicze ==========

HDR_RE = re.compile(r"^(#{1,3})\s+(.+)$", flags=re.MULTILINE)

def slugify(name: str) -> str:
    s = re.sub(r"[^\w\s-]", "", name, flags=re.UNICODE).strip().lower()
    s = re.sub(r"[\s_-]+", "-", s)
    return s or "podlinkowany"

def split_markdown_sections(text: str) -> List[Dict]:
    """
    Dzieli tekst po nag≈Ç√≥wkach # / ## / ###.
    Zwraca listƒô: [{"title": "...", "content": "..."}].
    Je≈ºeli brak nag≈Ç√≥wk√≥w -> 1 sekcja z tytu≈Çem z 1. linii albo z pierwszych s≈Ç√≥w.
    """
    positions = [(m.start(), m.group(1), m.group(2).strip()) for m in HDR_RE.finditer(text)]
    sections = []
    if not positions:
        head_line = text.strip().splitlines()[0] if text.strip() else "notatka"
        title = head_line.lstrip("# ").strip() or "notatka"
        sections.append({"title": title, "content": text})
        return sections

    positions.append((len(text), "", ""))  # sentinel
    for i in range(len(positions)-1):
        start, lvl, title = positions[i]
        end, _, _ = positions[i+1]
        block = text[start:end].rstrip()
        title = title or "sekcja"
        sections.append({"title": title, "content": block.strip() + "\n"})
    return sections

def count_total_chunks_multi(sections: List[Dict]) -> int:
    total = 0
    for s in sections:
        total += count_total_chunks(s["content"])
    return total

def split_keep_delimiters(text: str, pattern: re.Pattern) -> List[Tuple[str, bool]]:
    """
    Dzieli tekst na segmenty: [(segment, is_protected), ...]
    is_protected=True oznacza fragment, kt√≥rego nie modyfikujemy (np. kod, link MD).
    """
    pro
