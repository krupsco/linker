import os
import re
import json
from typing import Dict, List, Tuple

import streamlit as st

# --- OpenAI SDK (>=1.0) ---
try:
    from openai import OpenAI
except ImportError:
    st.error("Brak pakietu 'openai'. Upewnij siÄ™, Å¼e jest w requirements.txt")
    st.stop()

# ========== Konfiguracja ==========
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")  # ğŸš€ szybszy model
MAX_CHARS_PER_CHUNK = 6500   # bezpieczny limit na chunk (przybliÅ¼enie)
CHUNK_OVERLAP = 200          # miÄ™kkie â€zakÅ‚adkiâ€ miÄ™dzy chunkami

# Pobranie klucza z secrets Streamlita
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))

if not OPENAI_API_KEY:
    st.warning("âš ï¸ Nie znaleziono OPENAI_API_KEY w secrets. "
               "Dodaj go do .streamlit/secrets.toml lub zmiennej Å›rodowiskowej.")
client = OpenAI(api_key=OPENAI_API_KEY)


# ========== Prompty ==========
SYSTEM_PROMPT_BASE = """JesteÅ› asystentem-edytorem tekstÃ³w naukowych w jÄ™zyku polskim.
Twoje zadanie: dodaÄ‡ linki Obsidiana w nawiasach [[ ]] do PODANEGO TEKSTU, bez redakcji treÅ›ci.
Stosuj Å›cisÅ‚e zasady:

1) Linkujemy WSZYSTKIE kluczowe nazwy wÅ‚asne: osoby (historyczne, dynastie), budowle (zamki, paÅ‚ace, koÅ›cioÅ‚y, wieÅ¼e), ulice/duÅ¼e place/topografiÄ™,
   instytucje (sejm, urzÄ™dy, komisje, muzea), wydarzenia (wojny, unie, bitwy, konfederacje, elekcje).
2) JeÅ›li forma w tekÅ›cie jest odmieniona (nie w mianowniku), uÅ¼yj aliasu: [[LEMMA | forma_z_tekstu]].
   JeÅ›li forma jest rÃ³wna mianownikowi, uÅ¼yj [[LEMMA]].
3) LEMMA (mianownik) dobieraj nastÄ™pujÄ…co:
   - osoby: peÅ‚ne â€ImiÄ™ Nazwiskoâ€ + numeracja/dynastia (np. â€Zygmunt III Wazaâ€, â€Janusz I Starszyâ€).
   - rody/dynastie: liczba mnoga (â€Wazowieâ€, â€Piastowie mazowieccyâ€).
   - budowle: oficjalna polska nazwa (np. â€Zamek KrÃ³lewski w Warszawieâ€, â€WieÅ¼a Grodzkaâ€, â€DwÃ³r Wielkiâ€).
   - wydarzenia: forma sÅ‚ownikowa (np. â€Unia lubelskaâ€, â€Potop szwedzkiâ€, â€Bitwa pod KÅ‚uszynemâ€).
   - instytucje i topografia: forma sÅ‚ownikowa z kwalifikatorami miejsca (np. â€Kolegiata Å›w. Jana w Warszawieâ€, â€Plac Zamkowyâ€).
4) NIE modyfikuj istniejÄ…cych [[wikilinkÃ³w]], linkÃ³w Markdown, obrazkÃ³w `![]()`, tytuÅ‚Ã³w obrazÃ³w/cytatÃ³w w nawiasach, ani treÅ›ci blokÃ³w kodu.
5) Zachowaj oryginalne odstÄ™py, interpunkcjÄ™ i diakrytykÄ™. Nie skracaj, nie poprawiaj stylu.
6) JeÅ¼eli dany byt byÅ‚ juÅ¼ zmapowany w KNOWN_ENTITIES (lista poniÅ¼ej), uÅ¼ywaj dokÅ‚adnie tej lemmy (spÃ³jnoÅ›Ä‡ caÅ‚ego dokumentu).
7) Nie linkuj zwykÅ‚ych rzeczownikÃ³w pospolitych ani zbyt ogÃ³lnych pojÄ™Ä‡.

ZWROT:
- Najpierw zwrÃ³Ä‡ WYÅÄ„CZNIE przetworzony tekst.
- NastÄ™pnie w nowej linii daj separator:
-----ENTITY_MAP_JSON-----
i JSON z listÄ… wykrytych/aktualizowanych bytÃ³w (pole "entities": [{surface, lemma, type}...]).
- ZakoÅ„cz:
-----END-----

KNOWN_ENTITIES (priorytet spÃ³jnoÅ›ci):
"""

# typowa heurystyka wykrywania blokÃ³w kodu/obrazkÃ³w/linkÃ³w: nie zmieniamy ich
FENCE_RE = re.compile(r"(```.*?```|`.*?`)", re.DOTALL)
MARKDOWN_LINK_OR_IMAGE_RE = re.compile(r"(!?\[[^\]]*\]\([^)]+\))")


# ========== Pomocnicze ==========

def count_total_chunks(full_text: str) -> int:
    segments = compound_protection(full_text)
    total = 0
    for seg, protected in segments:
        if protected or not seg.strip():
            continue
        total += len(chunk_text_by_paragraphs(seg, MAX_CHARS_PER_CHUNK, CHUNK_OVERLAP))
    return total
    
def split_keep_delimiters(text: str, pattern: re.Pattern) -> List[Tuple[str, bool]]:
    """
    Dzieli tekst na segmenty: [(segment, is_protected), ...]
    is_protected=True oznacza fragment, ktÃ³rego nie modyfikujemy (np. kod, link MD).
    """
    protected = []
    last = 0
    for m in pattern.finditer(text):
        # fragment zwykÅ‚y
        if m.start() > last:
            protected.append((text[last:m.start()], False))
        # fragment chroniony
        protected.append((m.group(0), True))
        last = m.end()
    if last < len(text):
        protected.append((text[last:], False))
    return protected


def compound_protection(text: str) -> List[Tuple[str, bool]]:
    """Zabezpiecza jednoczeÅ›nie bloki kodu i konstrukcje linkÃ³w/obrazkÃ³w Markdown."""
    segs = split_keep_delimiters(text, FENCE_RE)
    out = []
    for seg, prot in segs:
        if prot:
            out.append((seg, True))
        else:
            # wewnÄ…trz zwykÅ‚ego segmentu zabezpiecz dodatkowo ![]() i []()
            sub = split_keep_delimiters(seg, MARKDOWN_LINK_OR_IMAGE_RE)
            out.extend(sub)
    return out


def chunk_text_by_paragraphs(text: str, max_chars: int, overlap: int) -> List[str]:
    paras = text.split("\n\n")
    chunks = []
    cur = []
    cur_len = 0
    for p in paras:
        p_block = (p + "\n\n")
        if cur_len + len(p_block) > max_chars and cur:
            chunk_text = "".join(cur).rstrip() + "\n"
            chunks.append(chunk_text)
            # zakÅ‚adka (weÅº koÅ„cÃ³wkÄ™ poprzedniego chunku)
            tail = chunk_text[-overlap:] if overlap > 0 else ""
            cur = [tail, p_block]
            cur_len = len(tail) + len(p_block)
        else:
            cur.append(p_block)
            cur_len += len(p_block)
    if cur:
        chunks.append("".join(cur))
    return chunks


def call_openai_linker(raw_text: str, known_entities: Dict[str, str], temperature: float = 0.1) -> Tuple[str, List[Dict]]:
    """
    WywoÅ‚uje model: zwraca (linked_text, entities_list)
    known_entities: dict lemma->list_of_known_surfaces (przekazujemy w prompt)
    """
    known_list = []
    for lemma, surfaces in known_entities.items():
        if isinstance(surfaces, list):
            known_list.append({"lemma": lemma, "surfaces": surfaces})
        else:
            known_list.append({"lemma": lemma, "surfaces": [surfaces]})

    sys = SYSTEM_PROMPT_BASE + json.dumps(known_list, ensure_ascii=False, indent=2)
    messages = [
        {"role": "system", "content": sys},
        {"role": "user", "content": raw_text}
    ]
    resp = client.chat.completions.create(
        model=MODEL,
        temperature=temperature,
        messages=messages
    )
    content = resp.choices[0].message.content

    # Parsowanie wyniku i mapy
    if "-----ENTITY_MAP_JSON-----" in content and "-----END-----" in content:
        text_part, json_part = content.split("-----ENTITY_MAP_JSON-----", 1)
        json_str = json_part.split("-----END-----", 1)[0].strip()
        try:
            ent_data = json.loads(json_str)
            entities = ent_data.get("entities", [])
        except Exception:
            entities = []
        return text_part.strip(), entities
    else:
        # fallback: brak JSON-a
        return content.strip(), []


def update_known_entities(known: Dict[str, List[str]], new_items: List[Dict]) -> Dict[str, List[str]]:
    for item in new_items:
        lemma = item.get("lemma")
        surface = item.get("surface")
        if not lemma or not surface:
            continue
        if lemma not in known:
            known[lemma] = []
        if surface not in known[lemma]:
            known[lemma].append(surface)
    return known


def process_text(full_text: str, temperature: float = 0.1) -> Tuple[str, Dict[str, List[str]]]:
    """
    CaÅ‚oÅ›ciowy pipeline:
    - chronimy kod/linki MD
    - chunkujemy zwykÅ‚e segmenty
    - wywoÅ‚ujemy model per chunk z narastajÄ…cÄ… mapÄ… encji
    - skÅ‚adamy wynik
    - AKTUALIZUJEMY pasek postÄ™pu Streamlit
    """
    segments = compound_protection(full_text)
    known_entities: Dict[str, List[str]] = {}
    out_segments = []

    # Pasek postÄ™pu (globalnie wszystkie chunki)
    total_chunks = count_total_chunks(full_text)
    done_chunks = 0
    progress = st.progress(0, text="PrzygotowujÄ™â€¦")
    status_placeholder = st.empty()

    for seg, protected in segments:
        if protected or not seg.strip():
            out_segments.append(seg)
            continue

        chunks = chunk_text_by_paragraphs(seg, MAX_CHARS_PER_CHUNK, CHUNK_OVERLAP)
        processed_parts = []
        for i, ch in enumerate(chunks, start=1):
            with st.spinner(f"Przetwarzam fragment {done_chunks + 1}/{total_chunks}â€¦"):
                linked, ents = call_openai_linker(ch, known_entities, temperature=temperature)
                processed_parts.append(linked)
                known_entities = update_known_entities(known_entities, ents)
            done_chunks += 1
            pct = int((done_chunks / max(total_chunks, 1)) * 100)
            progress.progress(pct, text=f"PostÄ™p: {pct}% ({done_chunks}/{total_chunks})")

        out_segments.append("".join(processed_parts))

    progress.progress(100, text="ZakoÅ„czono âœ…")
    status_placeholder.success("Przetwarzanie ukoÅ„czone.")
    return "".join(out_segments), known_entities



# ========== UI Streamlit ==========
st.set_page_config(page_title="Obsidian Linker (PL)", page_icon="ğŸ§­", layout="wide")

st.title("ğŸ§­ Obsidian Linker (PL)")
st.caption("Automatyczne dodawanie linkÃ³w [[ ]] (osoby, miejsca, wydarzenia) z aliasami w mianowniku â€” zgodnie z TwojÄ… logikÄ….")

with st.sidebar:
    st.subheader("Ustawienia")
    temp = st.slider("Temperatura (0 = bardzo zachowawczo)", 0.0, 1.0, 0.1, 0.05)
    st.divider()
    st.markdown("**Model**")
    st.code(MODEL)
    st.markdown("**Limit chunku**")
    st.code(f"{MAX_CHARS_PER_CHUNK} znakÃ³w")
    st.markdown("**Overlap**")
    st.code(f"{CHUNK_OVERLAP} znakÃ³w")
    st.divider()
    st.markdown("ğŸ” Klucz OpenAI pobierany z `st.secrets['OPENAI_API_KEY']`.")

st.markdown("### WejÅ›cie")
sample = st.toggle("Wstaw przykÅ‚adowy fragment", value=False)
default_text = ""
if sample:
    default_text = (
        "Pierwszym krÃ³lem polskim goszczÄ…cym w Zamku Warszawskim byÅ‚ WÅ‚adysÅ‚aw JagieÅ‚Å‚o. "
        "W 1526 roku Zygmunt I przejÄ…Å‚ Mazowsze po Å›mierci ksiÄ…Å¼Ä…t Janusza i StanisÅ‚awa. "
        "W 1569 roku Unia lubelska wyznaczyÅ‚a WarszawÄ™ i Zamek na staÅ‚e miejsce obrad sejmu."
    )

input_text = st.text_area("Wklej tekst (.md/.txt, bez limitu dÅ‚ugoÅ›ci â€“ aplikacja pociÄ…gnie w czÄ™Å›ciach):",
                          value=default_text, height=260)

uploaded = st.file_uploader("â€¦lub wgraj plik .md / .txt", type=["md", "txt"])
if uploaded is not None:
    input_text = uploaded.read().decode("utf-8")

colA, colB = st.columns([1,1])

with colA:
    run = st.button("ğŸš€ PrzetwÃ³rz", type="primary")

with colB:
    clear = st.button("ğŸ§¹ WyczyÅ›Ä‡ pamiÄ™Ä‡ encji tej sesji")

if "known_entities_session" not in st.session_state or clear:
    st.session_state.known_entities_session = {}

if run:
    if not input_text.strip():
        st.warning("Wklej tekst lub wgraj plik.")
        st.stop()
    if not OPENAI_API_KEY:
        st.error("Brak OPENAI_API_KEY. UzupeÅ‚nij `.streamlit/secrets.toml`.")
        st.stop()

    # Proces
    linked_text, new_map = process_text(input_text, temperature=temp)

    # Zaktualizuj pamiÄ™Ä‡ sesji (umoÅ¼liwia spÃ³jnoÅ›Ä‡ miÄ™dzy kolejnymi przetworzeniami)
    for lemma, surfaces in new_map.items():
        if lemma not in st.session_state.known_entities_session:
            st.session_state.known_entities_session[lemma] = []
        for s in surfaces:
            if s not in st.session_state.known_entities_session[lemma]:
                st.session_state.known_entities_session[lemma].append(s)

    st.success("Gotowe! PoniÅ¼ej wynik i mapa encji.")
    st.markdown("### Wynik (`.md`)")
    st.text_area("Podlinkowany tekst", value=linked_text, height=320)

    # Pobranie
    st.download_button("â¬‡ï¸ Pobierz jako Markdown", data=linked_text.encode("utf-8"),
                       file_name="podlinkowany.md", mime="text/markdown")

    st.markdown("### Mapa encji (dla spÃ³jnoÅ›ci i debugowania)")
    st.json(st.session_state.known_entities_session)

else:
    st.info("Ustaw parametry, wklej tekst i kliknij **PrzetwÃ³rz**. "
            "Aplikacja zadba o aliasy w mianowniku i spÃ³jnoÅ›Ä‡ nazw w caÅ‚ym dokumencie.")
